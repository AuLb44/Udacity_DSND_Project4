{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project analyzes user interactions with articles on the IBM Watson Studio platform to build recommendation systems. The goal is to recommend articles to users based on their previous interactions and the behavior of similar users.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Exploratory Data Analysis](#eda)\n",
    "2. [Rank Based Recommendations](#rank)\n",
    "3. [User-User Based Collaborative Filtering](#user-user)\n",
    "4. [Content Based Recommendations](#content-based)\n",
    "5. [Matrix Factorization](#matrix-fact)\n",
    "6. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to understand the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Part I: Exploratory Data Analysis\n",
    "\n",
    "Before building recommendation systems, let's explore the data to understand:\n",
    "- The distribution of how many articles a user interacts with\n",
    "- The number of unique articles that have an interaction\n",
    "- The number of unique users in the dataset\n",
    "- The most viewed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(f\"Shape of df: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check article content data\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What is the distribution of how many articles a user interacts with in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of user interactions\n",
    "user_interactions = df.groupby('email')['article_id'].count()\n",
    "user_interactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(user_interactions, bins=50)\n",
    "plt.xlabel('Number of Article Interactions')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of User-Article Interactions')\n",
    "plt.show()\n",
    "\n",
    "# Calculate required statistics\n",
    "median_val = user_interactions.median()\n",
    "max_views_by_user = user_interactions.max()\n",
    "\n",
    "print(f\"Median number of interactions per user: {median_val}\")\n",
    "print(f\"Maximum number of interactions by a user: {max_views_by_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Explore the number of unique articles and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key statistics\n",
    "unique_articles = df['article_id'].nunique()\n",
    "total_articles = df.shape[0]  # Total interactions\n",
    "unique_users = df['email'].nunique()\n",
    "user_article_interactions = df.shape[0]\n",
    "\n",
    "print(f\"Number of unique articles: {unique_articles}\")\n",
    "print(f\"Number of user-article interactions: {user_article_interactions}\")\n",
    "print(f\"Number of unique users: {unique_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Which article has the most interactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most viewed article\n",
    "article_views = df.groupby('article_id')['email'].count().sort_values(ascending=False)\n",
    "most_viewed_article_id = article_views.index[0]\n",
    "max_views = article_views.iloc[0]\n",
    "\n",
    "print(f\"Most viewed article ID: {most_viewed_article_id}\")\n",
    "print(f\"Number of views: {max_views}\")\n",
    "print(f\"\\nTop 10 most viewed articles:\")\n",
    "print(article_views.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create User-ID Mapping\n",
    "\n",
    "We'll create a mapping from email addresses to user IDs for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user_id column by mapping email to a unique numeric ID\n",
    "def email_mapper(df=df):\n",
    "    \"\"\"\n",
    "    Maps email addresses to user IDs\n",
    "    \n",
    "    Args:\n",
    "        df: pandas dataframe with email column\n",
    "    \n",
    "    Returns:\n",
    "        email_encoded: list of user_ids corresponding to emails in df\n",
    "    \"\"\"\n",
    "    coded_dict = {\n",
    "        email: num \n",
    "        for num, email in enumerate(df['email'].unique(), start=1)\n",
    "    }\n",
    "    return [coded_dict[val] for val in df['email']]\n",
    "\n",
    "df['user_id'] = email_mapper(df)\n",
    "del df['email']\n",
    "\n",
    "# Show the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rank'></a>\n",
    "## Part II: Rank Based Recommendations\n",
    "\n",
    "For new users or cold-start scenarios, we can recommend the most popular articles. This approach recommends articles based on the number of interactions they have received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    \"\"\"\n",
    "    Returns the top n article titles with the most interactions\n",
    "    \n",
    "    Args:\n",
    "        n: int, number of top articles to return\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        top_articles: list of top n article titles\n",
    "    \"\"\"\n",
    "    # Get article counts\n",
    "    article_counts = df.groupby('title')['user_id'].count().sort_values(ascending=False)\n",
    "    top_articles = list(article_counts.index[:n])\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    \"\"\"\n",
    "    Returns the top n article IDs with the most interactions\n",
    "    \n",
    "    Args:\n",
    "        n: int, number of top articles to return\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        top_articles: list of top n article IDs as strings\n",
    "    \"\"\"\n",
    "    # Get article counts by article_id\n",
    "    article_counts = df.groupby('article_id')['user_id'].count().sort_values(ascending=False)\n",
    "    top_articles = list(article_counts.index[:n].astype(str))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Test the functions\n",
    "print(\"Top 10 Article Titles:\")\n",
    "top_10_titles = get_top_articles(10)\n",
    "for i, title in enumerate(top_10_titles, 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n",
    "print(\"\\nTop 10 Article IDs:\")\n",
    "top_10_ids = get_top_article_ids(10)\n",
    "print(top_10_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='user-user'></a>\n",
    "## Part III: User-User Based Collaborative Filtering\n",
    "\n",
    "In this section, we build a collaborative filtering system. We'll find similar users based on their article interactions and recommend articles that similar users have interacted with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df, fill_value=0):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix where 1 indicates an interaction\n",
    "    \n",
    "    Args:\n",
    "        df: pandas dataframe of user-item interactions\n",
    "        fill_value: value to fill for non-interactions (default 0)\n",
    "    \n",
    "    Returns:\n",
    "        user_item: pandas dataframe with users as rows and articles as columns\n",
    "    \"\"\"\n",
    "    # Create a user-item matrix with 1's for interactions\n",
    "    # fill_value is used by unstack() before binary conversion\n",
    "    user_item = df.groupby(['user_id', 'article_id'])['title'].count().unstack(fill_value=fill_value)\n",
    "    \n",
    "    # Convert to binary (1 for interaction, 0 for no interaction)\n",
    "    # Note: After this, all values become 0 or 1 regardless of fill_value\n",
    "    user_item = (user_item > 0).astype(int)\n",
    "    \n",
    "    return user_item\n",
    "\n",
    "# Create the user-item matrix\n",
    "user_item = create_user_item_matrix(df)\n",
    "\n",
    "print(f\"User-Item Matrix Shape: {user_item.shape}\")\n",
    "print(f\"Number of users: {user_item.shape[0]}\")\n",
    "print(f\"Number of articles: {user_item.shape[1]}\")\n",
    "user_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item, include_similarity=False):\n",
    "    \"\"\"\n",
    "    Finds users similar to a given user based on article interactions\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id to find similar users for\n",
    "        user_item: pandas dataframe, user-item matrix\n",
    "        include_similarity: bool, whether to include similarity scores in output\n",
    "    \n",
    "    Returns:\n",
    "        similar_users: numpy array of similar user_ids sorted by similarity,\n",
    "                      or list of lists [[user_id, similarity], ...] if include_similarity=True\n",
    "    \"\"\"\n",
    "    # Check if user exists\n",
    "    if user_id not in user_item.index:\n",
    "        if include_similarity:\n",
    "            return []\n",
    "        return np.array([])\n",
    "    \n",
    "    # Calculate similarity using dot product (number of common articles)\n",
    "    user_vector = user_item.loc[user_id].values.reshape(1, -1)\n",
    "    similarities = user_item.dot(user_vector.T).flatten()\n",
    "    \n",
    "    # Sort by similarity (excluding the user itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1]\n",
    "    similar_users = user_item.index[similar_indices].values\n",
    "    similar_scores = similarities[similar_indices]\n",
    "    \n",
    "    # Remove the user itself\n",
    "    mask = similar_users != user_id\n",
    "    similar_users = similar_users[mask]\n",
    "    similar_scores = similar_scores[mask]\n",
    "    \n",
    "    if include_similarity:\n",
    "        return [[int(user), float(score)] for user, score in zip(similar_users, similar_scores)]\n",
    "    \n",
    "    return similar_users\n",
    "\n",
    "# Test the function\n",
    "test_user = 1\n",
    "similar = find_similar_users(test_user)\n",
    "print(f\"Most similar users to user {test_user}: {similar[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    \"\"\"\n",
    "    Get article names from article IDs\n",
    "    \n",
    "    Args:\n",
    "        article_ids: list of article_ids\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        article_names: list of article titles\n",
    "    \"\"\"\n",
    "    article_names = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        # Get the title for this article_id\n",
    "        title = df[df['article_id'] == float(article_id)]['title'].iloc[0]\n",
    "        article_names.append(title)\n",
    "    \n",
    "    return article_names\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item, df=df):\n",
    "    \"\"\"\n",
    "    Get articles that a user has interacted with\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id\n",
    "        user_item: pandas dataframe, user-item matrix\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        article_ids: list of article_ids the user has interacted with\n",
    "        article_names: list of article titles corresponding to article_ids\n",
    "    \"\"\"\n",
    "    # Get articles where user has interacted (value = 1)\n",
    "    if user_id not in user_item.index:\n",
    "        return [], []\n",
    "    \n",
    "    user_row = user_item.loc[user_id]\n",
    "    article_ids = list(user_row[user_row == 1].index.astype(float))\n",
    "    \n",
    "    # Get article names\n",
    "    article_names = get_article_names(article_ids, df)\n",
    "    \n",
    "    # Convert article_ids to strings for consistency\n",
    "    article_ids = [str(int(aid)) for aid in article_ids]\n",
    "    \n",
    "    return article_ids, article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    \"\"\"\n",
    "    Get users sorted by similarity to the given user\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id\n",
    "        df: pandas dataframe of user-item interactions\n",
    "        user_item: pandas dataframe, user-item matrix\n",
    "    \n",
    "    Returns:\n",
    "        neighbors_df: pandas dataframe of similar users with similarity scores\n",
    "    \"\"\"\n",
    "    # Find similar users with similarity scores\n",
    "    similar_users_with_sim = find_similar_users(user_id, user_item, include_similarity=True)\n",
    "    \n",
    "    # Check if user exists\n",
    "    if not similar_users_with_sim:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract user ids and similarities\n",
    "    user_ids = [u[0] for u in similar_users_with_sim]\n",
    "    similarities = [u[1] for u in similar_users_with_sim]\n",
    "    \n",
    "    # Calculate number of interactions for each user\n",
    "    num_interactions = [user_item.loc[uid].sum() for uid in user_ids]\n",
    "    \n",
    "    # Create dataframe\n",
    "    neighbors_df = pd.DataFrame({\n",
    "        'neighbor_id': user_ids,\n",
    "        'similarity': similarities,\n",
    "        'num_interactions': num_interactions\n",
    "    })\n",
    "    \n",
    "    # Sort by similarity (descending) then by num_interactions (descending)\n",
    "    neighbors_df = neighbors_df.sort_values(['similarity', 'num_interactions'], ascending=False)\n",
    "    \n",
    "    return neighbors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-User Recommendation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs(user_id, m=10):\n",
    "    \"\"\"\n",
    "    Recommend articles to a user based on similar users' interactions\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id to make recommendations for\n",
    "        m: int, number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        recs: list of article_ids to recommend\n",
    "    \"\"\"\n",
    "    # Get articles the user has already seen\n",
    "    user_articles_ids, user_articles_names = get_user_articles(user_id, user_item, df)\n",
    "    user_articles = set(user_articles_ids)\n",
    "    \n",
    "    # Find similar users\n",
    "    similar_users = find_similar_users(user_id, user_item)\n",
    "    \n",
    "    # Get articles from similar users\n",
    "    rec_articles = []\n",
    "    article_counts = {}\n",
    "    \n",
    "    for sim_user in similar_users:\n",
    "        sim_user_articles_ids, _ = get_user_articles(sim_user, user_item, df)\n",
    "        \n",
    "        for article_id in sim_user_articles_ids:\n",
    "            if article_id not in user_articles:\n",
    "                article_counts[article_id] = article_counts.get(article_id, 0) + 1\n",
    "    \n",
    "    # Sort by popularity among similar users\n",
    "    sorted_articles = sorted(article_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    recs = [article_id for article_id, count in sorted_articles[:m]]\n",
    "    \n",
    "    # If we don't have enough recommendations, add popular articles\n",
    "    if len(recs) < m:\n",
    "        top_articles = get_top_article_ids(m * 2, df)\n",
    "        for article_id in top_articles:\n",
    "            if article_id not in user_articles and article_id not in recs:\n",
    "                recs.append(article_id)\n",
    "                if len(recs) >= m:\n",
    "                    break\n",
    "    \n",
    "    return recs[:m]\n",
    "\n",
    "# Test the function\n",
    "test_recs = user_user_recs(1, m=10)\n",
    "print(f\"Recommendations for user 1: {test_recs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_article_unique_counts(article_ids, user_item=user_item):\n",
    "    \"\"\"\n",
    "    Get articles ranked by the number of unique users who interacted with them\n",
    "    \n",
    "    Args:\n",
    "        article_ids: list of article_ids to rank\n",
    "        user_item: pandas dataframe, user-item matrix\n",
    "    \n",
    "    Returns:\n",
    "        ranked_article_unique_counts: list of tuples (article_id, unique_user_count)\n",
    "                                      sorted by unique_user_count in descending order\n",
    "    \"\"\"\n",
    "    # Count unique users for each article in the provided list\n",
    "    article_counts = []\n",
    "    \n",
    "    for article_id in article_ids:\n",
    "        # Convert to float for consistency with user_item columns\n",
    "        article_id_float = float(article_id)\n",
    "        \n",
    "        # Check if article exists in user_item matrix\n",
    "        if article_id_float in user_item.columns:\n",
    "            # Count users who interacted with this article (value = 1)\n",
    "            count = user_item[article_id_float].sum()\n",
    "            article_counts.append([int(article_id_float), int(count)])\n",
    "    \n",
    "    # Sort by count (descending)\n",
    "    ranked_article_unique_counts = sorted(article_counts, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_article_unique_counts\n",
    "\n",
    "# Test\n",
    "test_articles = [1320, 232, 844]\n",
    "ranked_articles = get_ranked_article_unique_counts(test_articles, user_item)\n",
    "print(f\"Ranked articles {test_articles}:\")\n",
    "print(ranked_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs_part2(user_id, m=10):\n",
    "    \"\"\"\n",
    "    Improved version of user_user_recs with better handling of edge cases\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id to make recommendations for\n",
    "        m: int, number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        recs: list of article_ids to recommend\n",
    "        rec_names: list of article names corresponding to recs\n",
    "    \"\"\"\n",
    "    # For new users, return most popular articles\n",
    "    if user_id not in user_item.index:\n",
    "        recs = get_top_article_ids(m, df)\n",
    "        rec_names = get_article_names(recs, df)\n",
    "        return recs, rec_names\n",
    "    \n",
    "    # Use the original user_user_recs function\n",
    "    recs = user_user_recs(user_id, m)\n",
    "    rec_names = get_article_names(recs, df)\n",
    "    \n",
    "    return recs, rec_names\n",
    "\n",
    "# Test\n",
    "test_recs, test_names = user_user_recs_part2(1, m=5)\n",
    "print(\"Recommendations for user 1:\")\n",
    "for i, (rec_id, name) in enumerate(zip(test_recs, test_names), 1):\n",
    "    print(f\"{i}. {name} (ID: {rec_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Required Variables for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate required variables for grading\n",
    "\n",
    "# Most similar user to user 1\n",
    "similar_to_1 = find_similar_users(1, user_item)\n",
    "user1_most_sim = similar_to_1[0] if len(similar_to_1) > 0 else None\n",
    "\n",
    "# 6th most similar user to user 2\n",
    "similar_to_2 = find_similar_users(2, user_item)\n",
    "user2_6th_sim = similar_to_2[5] if len(similar_to_2) > 5 else None\n",
    "\n",
    "# 10th most similar user to user 131\n",
    "similar_to_131 = find_similar_users(131, user_item)\n",
    "user131_10th_sim = similar_to_131[9] if len(similar_to_131) > 9 else None\n",
    "\n",
    "# Recommendations for a new user (user not in dataset)\n",
    "new_user_id = max(user_item.index) + 1\n",
    "new_user_recs = get_top_article_ids(10, df)\n",
    "\n",
    "print(f\"Most similar user to user 1: {user1_most_sim}\")\n",
    "print(f\"6th most similar user to user 2: {user2_6th_sim}\")\n",
    "print(f\"10th most similar user to user 131: {user131_10th_sim}\")\n",
    "print(f\"\\nRecommendations for new user: {new_user_recs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='content-based'></a>\n",
    "## Part IV: Content Based Recommendations\n",
    "\n",
    "In this section, we use article content (text) to make recommendations. We'll use TF-IDF to vectorize article text, apply dimensionality reduction with LSA, and cluster similar articles using K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Article Content Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check article content data\n",
    "print(\"Article content columns:\", df_content.columns.tolist())\n",
    "print(f\"\\nShape: {df_content.shape}\")\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare text data\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text for processing\n",
    "    \n",
    "    Args:\n",
    "        text: string\n",
    "    \n",
    "    Returns:\n",
    "        cleaned text string\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combine title and doc_body for content\n",
    "df_content['content'] = df_content['doc_full_name'].fillna('') + ' ' + df_content['doc_description'].fillna('')\n",
    "df_content['content'] = df_content['content'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty content\n",
    "df_content = df_content[df_content['content'].str.len() > 0]\n",
    "\n",
    "print(f\"Articles with content: {len(df_content)}\")\n",
    "df_content[['article_id', 'doc_full_name', 'content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization and LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix\n",
    "# Parameters:\n",
    "#   - max_features=1000: Limit vocabulary to top 1000 terms to reduce dimensionality\n",
    "#   - stop_words='english': Remove common English words that don't add meaning\n",
    "#   - max_df=0.8: Ignore terms that appear in more than 80% of documents\n",
    "#   - min_df=2: Ignore terms that appear in fewer than 2 documents\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english', max_df=0.8, min_df=2)\n",
    "tfidf_matrix = tfidf.fit_transform(df_content['content'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Apply LSA (Latent Semantic Analysis) for dimensionality reduction\n",
    "# Use min() to ensure n_components doesn't exceed matrix dimensions\n",
    "n_components = min(100, tfidf_matrix.shape[0] - 1, tfidf_matrix.shape[1] - 1)\n",
    "lsa = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"LSA matrix shape: {lsa_matrix.shape}\")\n",
    "print(f\"Explained variance ratio: {lsa.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "# n_clusters=20: Group articles into 20 thematic clusters\n",
    "# random_state=42: Ensure reproducibility\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "df_content['cluster'] = kmeans.fit_predict(lsa_matrix)\n",
    "\n",
    "print(f\"Number of articles per cluster:\")\n",
    "print(df_content['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Recommendation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(article_id, df=df):\n",
    "    \"\"\"\n",
    "    Find similar articles based on title similarity (using clusters)\n",
    "    \n",
    "    Args:\n",
    "        article_id: float or int, the article_id\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        similar_articles: list of similar article_ids as strings\n",
    "    \"\"\"\n",
    "    # Check if article exists\n",
    "    article_id = float(article_id)\n",
    "    \n",
    "    # Get the article's title\n",
    "    article_titles = df[df['article_id'] == article_id]['title']\n",
    "    if len(article_titles) == 0:\n",
    "        return []\n",
    "    \n",
    "    article_title = article_titles.iloc[0]\n",
    "    \n",
    "    # Find all articles with the same title (title cluster)\n",
    "    same_title_articles = df[df['title'] == article_title]['article_id'].unique()\n",
    "    \n",
    "    # Remove the input article_id from the list\n",
    "    similar_articles = [str(int(aid)) for aid in same_title_articles if aid != article_id]\n",
    "    \n",
    "    return similar_articles\n",
    "\n",
    "# Test the function\n",
    "test_article_id = df['article_id'].iloc[0]\n",
    "similar = get_similar_articles(test_article_id, df)\n",
    "print(f\"Articles similar to {test_article_id}:\")\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_recs(article_id, n, df=df):\n",
    "    \"\"\"\n",
    "    Make content-based recommendations for an article\n",
    "    Returns similar articles ranked by popularity\n",
    "    \n",
    "    Args:\n",
    "        article_id: int or float, the article_id\n",
    "        n: int, number of recommendations to return\n",
    "        df: pandas dataframe of user-item interactions\n",
    "    \n",
    "    Returns:\n",
    "        n_ranked_similar_articles: list of n similar article_ids ranked by popularity\n",
    "        n_ranked_article_names: list of article names corresponding to article_ids\n",
    "    \"\"\"\n",
    "    # Get similar articles (articles in same title cluster)\n",
    "    similar_article_ids = get_similar_articles(article_id, df)\n",
    "    \n",
    "    if not similar_article_ids:\n",
    "        # If no similar articles, return popular articles\n",
    "        top_ids = get_top_article_ids(n, df)\n",
    "        top_names = get_article_names([float(aid) for aid in top_ids], df)\n",
    "        return top_ids, top_names\n",
    "    \n",
    "    # Convert to int for get_ranked_article_unique_counts\n",
    "    # Note: similar_article_ids are strings, convert to int\n",
    "    similar_article_ids_int = [int(aid) for aid in similar_article_ids]\n",
    "    \n",
    "    # Rank by unique user counts\n",
    "    ranked_counts = get_ranked_article_unique_counts(similar_article_ids_int, user_item)\n",
    "    \n",
    "    # Extract article_ids and limit to n\n",
    "    n_ranked_similar_articles = [str(aid) for aid, count in ranked_counts[:n]]\n",
    "    \n",
    "    # Get article names\n",
    "    n_ranked_article_names = get_article_names([float(aid) for aid in n_ranked_similar_articles], df)\n",
    "    \n",
    "    return n_ranked_similar_articles, n_ranked_article_names\n",
    "\n",
    "# Test\n",
    "test_article_id = df['article_id'].iloc[0]\n",
    "content_recs_ids, content_recs_names = make_content_recs(test_article_id, n=5, df=df)\n",
    "print(f\"Content-based recommendations for article {test_article_id}:\")\n",
    "for i, (aid, name) in enumerate(zip(content_recs_ids, content_recs_names), 1):\n",
    "    print(f\"{i}. {name} (ID: {aid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='matrix-fact'></a>\n",
    "## Part V: Matrix Factorization (SVD)\n",
    "\n",
    "Finally, we use Singular Value Decomposition (SVD) to factorize the user-item matrix and make recommendations based on latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the user-item matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Convert to numpy array for SVD\n",
    "user_item_matrix = user_item.values\n",
    "\n",
    "# Choose number of latent factors\n",
    "k = 50\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = svds(user_item_matrix, k=k)\n",
    "\n",
    "# Convert sigma to diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Sigma shape: {sigma.shape}\")\n",
    "print(f\"Vt shape: {Vt.shape}\")\n",
    "\n",
    "# Reconstruct the matrix\n",
    "predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "predicted_ratings_df = pd.DataFrame(predicted_ratings, \n",
    "                                    index=user_item.index, \n",
    "                                    columns=user_item.columns)\n",
    "\n",
    "print(f\"\\nPredicted ratings shape: {predicted_ratings_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_similar_article_ids(article_id, vt, user_item=user_item, include_similarity=False):\n",
    "    \"\"\"\n",
    "    Find similar articles using SVD-based latent features\n",
    "    \n",
    "    Args:\n",
    "        article_id: float or int, the article_id\n",
    "        vt: numpy array, Vt matrix from SVD decomposition\n",
    "        user_item: pandas dataframe, user-item matrix\n",
    "        include_similarity: bool, whether to include similarity scores\n",
    "    \n",
    "    Returns:\n",
    "        similar_articles: list of similar article_ids as strings,\n",
    "                         or list of lists [[article_id, similarity], ...] if include_similarity=True\n",
    "    \"\"\"\n",
    "    article_id = float(article_id)\n",
    "    \n",
    "    # Check if article exists in the matrix\n",
    "    if article_id not in user_item.columns:\n",
    "        if include_similarity:\n",
    "            return []\n",
    "        return []\n",
    "    \n",
    "    # Get the article's latent feature vector from Vt\n",
    "    article_idx = list(user_item.columns).index(article_id)\n",
    "    article_vector = vt[:, article_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarity with all articles\n",
    "    # vt should be transposed to get shape (n_articles, k)\n",
    "    similarities = cosine_similarity(article_vector, vt.T).flatten()\n",
    "    \n",
    "    # Get indices of most similar articles (excluding the article itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Filter out the article itself\n",
    "    similar_indices = [idx for idx in similar_indices if user_item.columns[idx] != article_id]\n",
    "    \n",
    "    if include_similarity:\n",
    "        # Return list of [article_id, similarity]\n",
    "        result = []\n",
    "        for idx in similar_indices:\n",
    "            col_val = user_item.columns[idx]\n",
    "            # Convert to string, handling both float and int types\n",
    "            article_id_str = str(int(float(col_val)))\n",
    "            result.append([article_id_str, float(similarities[idx])])\n",
    "        return result\n",
    "    else:\n",
    "        # Get article_ids as strings\n",
    "        similar_articles = []\n",
    "        for idx in similar_indices:\n",
    "            col_val = user_item.columns[idx]\n",
    "            # Convert to string, handling both float and int types\n",
    "            article_id_str = str(int(float(col_val)))\n",
    "            similar_articles.append(article_id_str)\n",
    "        return similar_articles\n",
    "\n",
    "# Test the function\n",
    "test_article = user_item.columns[0]\n",
    "svd_similar = get_svd_similar_article_ids(test_article, Vt, user_item)[:5]\n",
    "print(f\"SVD-based articles similar to {test_article}:\")\n",
    "print(svd_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_recommendations(user_id, m=10):\n",
    "    \"\"\"\n",
    "    Make recommendations using SVD-predicted ratings\n",
    "    \n",
    "    Args:\n",
    "        user_id: int, the user_id\n",
    "        m: int, number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        recs: list of recommended article_ids\n",
    "    \"\"\"\n",
    "    if user_id not in predicted_ratings_df.index:\n",
    "        # Return popular articles for new users\n",
    "        return get_top_article_ids(m, df)\n",
    "    \n",
    "    # Get user's predicted ratings\n",
    "    user_predictions = predicted_ratings_df.loc[user_id]\n",
    "    \n",
    "    # Get articles user hasn't interacted with\n",
    "    user_articles_ids, _ = get_user_articles(user_id, user_item, df)\n",
    "    user_articles = set(user_articles_ids)\n",
    "    \n",
    "    # Sort by predicted rating\n",
    "    sorted_predictions = user_predictions.sort_values(ascending=False)\n",
    "    \n",
    "    # Filter out articles user has already seen\n",
    "    recs = []\n",
    "    for article_id in sorted_predictions.index:\n",
    "        article_id_str = str(article_id)\n",
    "        if article_id_str not in user_articles:\n",
    "            recs.append(article_id_str)\n",
    "            if len(recs) >= m:\n",
    "                break\n",
    "    \n",
    "    return recs\n",
    "\n",
    "# Test\n",
    "svd_recs = svd_recommendations(1, m=5)\n",
    "print(f\"SVD-based recommendations for user 1: {svd_recs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## Part VI: Evaluation and Conclusion\n",
    "\n",
    "### Comparison of Recommendation Methods\n",
    "\n",
    "We have implemented four different recommendation approaches:\n",
    "\n",
    "1. **Rank-Based Recommendations**: Simple approach that recommends the most popular articles. Works well for new users (cold start) but doesn't personalize.\n",
    "\n",
    "2. **User-User Collaborative Filtering**: Finds similar users based on interaction patterns and recommends articles that similar users liked. Works well when we have sufficient user interaction data.\n",
    "\n",
    "3. **Content-Based Recommendations**: Uses article content (text) to find similar articles. Good for recommending articles similar to what a user has already read, even for articles with few interactions.\n",
    "\n",
    "4. **Matrix Factorization (SVD)**: Discovers latent features in user-item interactions to make predictions. Can capture complex patterns and make personalized recommendations.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- The dataset is sparse, with many users having only a few interactions\n",
    "- Popular articles have significantly more interactions than average\n",
    "- Different recommendation methods have different strengths for different scenarios\n",
    "\n",
    "### Recommendations for Deployment\n",
    "\n",
    "A hybrid approach would work best:\n",
    "- Use rank-based for completely new users\n",
    "- Use collaborative filtering for users with sufficient history\n",
    "- Use content-based to discover similar articles\n",
    "- Use SVD for personalized predictions when data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"RECOMMENDATION SYSTEM SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  - Total interactions: {user_article_interactions}\")\n",
    "print(f\"  - Unique users: {unique_users}\")\n",
    "print(f\"  - Unique articles: {unique_articles}\")\n",
    "print(f\"  - Median interactions per user: {median_val}\")\n",
    "print(f\"  - Max interactions by a user: {max_views_by_user}\")\n",
    "print(f\"  - Most viewed article ID: {most_viewed_article_id}\")\n",
    "print(f\"  - Views of most popular article: {max_views}\")\n",
    "\n",
    "print(f\"\\nRecommendation Models Implemented:\")\n",
    "print(f\"  1. Rank-Based Recommendations\")\n",
    "print(f\"  2. User-User Collaborative Filtering\")\n",
    "print(f\"  3. Content-Based Recommendations (TF-IDF + LSA + KMeans)\")\n",
    "print(f\"  4. Matrix Factorization (SVD)\")\n",
    "\n",
    "print(f\"\\nKey Variables for Testing:\")\n",
    "print(f\"  - user1_most_sim: {user1_most_sim}\")\n",
    "print(f\"  - user2_6th_sim: {user2_6th_sim}\")\n",
    "print(f\"  - user131_10th_sim: {user131_10th_sim}\")\n",
    "print(f\"  - new_user_recs (first 5): {new_user_recs[:5]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To further improve the recommendation system:\n",
    "\n",
    "1. **Implement A/B testing** to compare different recommendation strategies in production\n",
    "2. **Add temporal features** to account for article freshness and user behavior over time\n",
    "3. **Incorporate implicit feedback** like reading time, clicks, and scrolling behavior\n",
    "4. **Build a hybrid model** that combines multiple approaches with learned weights\n",
    "5. **Add diversity** to recommendations to avoid filter bubbles\n",
    "6. **Implement online learning** to update recommendations in real-time\n",
    "7. **Add evaluation metrics** like precision@k, recall@k, and NDCG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}